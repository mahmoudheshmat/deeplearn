{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and build a feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ])\n",
    "\n",
    "# download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3214, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# get data\n",
    "images, labels = next(iter(trainloader))\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get logits\n",
    "logits = model(images)\n",
    "\n",
    "# calculate loss with logits and labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss\n",
    "\n",
    "\n",
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3032, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation using autograd\n",
    "Autograd is a module to automatically calculate the gradients of tensors\n",
    "- for this, you'll need to have requires_grad = True    on a tensor so that PyTorch will keep track of operations on a tensor and calculate the gradients\n",
    "- by default it is true, to turn off, torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6506, -0.7098],\n",
      "        [ 0.0264, -0.0479]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there are no gradients set (as seen in cell above). To calculate the gradients, you need to run the .backward method on a variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [-0.0002, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0002],\n",
      "        [ 0.0013,  0.0013,  0.0013,  ...,  0.0013,  0.0013,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n",
      "        [-0.0033, -0.0033, -0.0033,  ..., -0.0033, -0.0033, -0.0033],\n",
      "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "- this package will update the weights with gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# optimizers require parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General process:\n",
    "1. make a forward pass through network: nn.Sequential()\n",
    "2. calculate loss with network output: nn.NLLLoss(logits, labels)\n",
    "3. perform backward pass with loss.backward() to calculate gradients\n",
    "4. step with optimizer to update the weights\n",
    "\n",
    "loop until satisfactory loss\n",
    "\n",
    "\n",
    "### One training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0184, -0.0115,  0.0199,  ..., -0.0251,  0.0303, -0.0120],\n",
      "        [ 0.0255, -0.0247,  0.0001,  ..., -0.0144,  0.0163,  0.0213],\n",
      "        [ 0.0186, -0.0026, -0.0095,  ..., -0.0220,  0.0059, -0.0039],\n",
      "        ...,\n",
      "        [-0.0300, -0.0080,  0.0151,  ..., -0.0011, -0.0192,  0.0162],\n",
      "        [ 0.0242,  0.0160,  0.0303,  ...,  0.0175,  0.0254,  0.0305],\n",
      "        [ 0.0306,  0.0236,  0.0309,  ..., -0.0220,  0.0331,  0.0172]],\n",
      "       requires_grad=True)\n",
      "Gradient  - tensor([[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        ...,\n",
      "        [-0.0071, -0.0071, -0.0071,  ..., -0.0071, -0.0071, -0.0071],\n",
      "        [ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033],\n",
      "        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# clear gradients, because they are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forawrd pass, backward pass, update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient  -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0184, -0.0115,  0.0199,  ..., -0.0251,  0.0303, -0.0120],\n",
      "        [ 0.0255, -0.0247,  0.0001,  ..., -0.0144,  0.0163,  0.0213],\n",
      "        [ 0.0186, -0.0026, -0.0095,  ..., -0.0220,  0.0059, -0.0039],\n",
      "        ...,\n",
      "        [-0.0300, -0.0079,  0.0152,  ..., -0.0011, -0.0191,  0.0163],\n",
      "        [ 0.0242,  0.0160,  0.0302,  ...,  0.0174,  0.0254,  0.0304],\n",
      "        [ 0.0306,  0.0236,  0.0309,  ..., -0.0220,  0.0331,  0.0172]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# take an update step and view the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training entire image set\n",
    "Need to loop through algorithm.\n",
    "- Each pass through the dataset is called an epoch\n",
    "- loop through trainloader to get training batches\n",
    "- each batch we'll do a training pass to calculate the loss, do a backwards pass and update the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8920165837954865\n",
      "Training loss: 0.8326479111716691\n",
      "Training loss: 0.5251231840742168\n",
      "Training loss: 0.42768748363515713\n",
      "Training loss: 0.38100168069224877\n"
     ]
    }
   ],
   "source": [
    "# feedforward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# loss function\n",
    "criterion = nn.NLLLoss()\n",
    "# update the model's weights with gradients\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5 # why 5?\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # clear gradients between training passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test out an image\n",
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model(img)\n",
    "    \n",
    "# output of network are logits; take softmax for probability\n",
    "ps = F.softmax(logits, dim=1)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
