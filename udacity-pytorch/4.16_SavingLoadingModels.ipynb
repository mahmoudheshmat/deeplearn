{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models\n",
    "\n",
    "This will be incredibly useful because you'll want to load previously trained models to use in making predictions or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# download and load training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample image\n",
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        # dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # flatten input tensor\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1) # no dropout on output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3..  Training Loss: 0.603..  Test Loss: 0.484..  Test Accuracy: 0.830.. \n",
      "Epoch: 2/3..  Training Loss: 0.481..  Test Loss: 0.447..  Test Accuracy: 0.840.. \n",
      "Epoch: 3/3..  Training Loss: 0.454..  Test Loss: 0.420..  Test Accuracy: 0.848.. \n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "#model.cuda()\n",
    "images, labels = next(iter(trainloader))\n",
    "ps = torch.exp(model(images))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 3\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        #images.cuda()\n",
    "        #labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # turn off gradients for validation to save memory and computation\n",
    "        with torch.no_grad():\n",
    "            model.eval() # turns off dropout\n",
    "            for images, labels in testloader:\n",
    "                log_ps = model(images)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        model.train() # turns dropout back on\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "        \n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"Test Accuracy: {:.3f}.. \".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we don't want to train the model every time we need to use it, we can save the trained network for later loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Classifier(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for PyTorch networks are stored in the model's state_dict. Above you'll see the weight and bias matrices for each of the layers. Now, save the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This state dict can then be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the state dict into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind some requirements:\n",
    "Loading the state dict only works if the model architecture is the same as the checkpoint architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Classifier' object has no attribute 'hidden_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1d54688e17bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint = {'input_size': 784,\n\u001b[0;32m      2\u001b[0m               \u001b[1;34m'output_size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m               \u001b[1;34m'hidden_layers'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m               'state_dict': model.state_dict()}\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    533\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 535\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Classifier' object has no attribute 'hidden_layers'"
     ]
    }
   ],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
